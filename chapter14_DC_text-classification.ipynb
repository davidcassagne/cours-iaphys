{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidcassagne/cours-iaphys/blob/main/chapter14_DC_text-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5w6ZnFMGHm-"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Third Edition](https://www.manning.com/books/deep-learning-with-python-third-edition). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "The book's contents are available online at [deeplearningwithpython.io](https://deeplearningwithpython.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCB29XWEGHm_"
      },
      "outputs": [],
      "source": [
        "# Ajout DC\n",
        "# La ligne suivante est inutile dans Google Colab\n",
        "# !pip install keras keras-hub --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mQbz0yOGHnA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YsiEkuhoGHnA"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def backend(line, cell):\n",
        "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
        "    if current == required:\n",
        "        get_ipython().run_cell(cell)\n",
        "    else:\n",
        "        print(\n",
        "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
        "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDNUEcKCGHnA"
      },
      "source": [
        "## Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAUNSp9WGHnA"
      },
      "source": [
        "### A brief history of natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IvwzbK9GHnA"
      },
      "source": [
        "### Preparing text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gseyiyUGHnA"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "def split_chars(text):\n",
        "    return re.findall(r\".\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5ZMwrW_GHnB"
      },
      "outputs": [],
      "source": [
        "chars = split_chars(\"The quick brown fox jumped over the lazy dog.\")\n",
        "chars[:12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3naX10jnGHnB"
      },
      "outputs": [],
      "source": [
        "def split_words(text):\n",
        "    return re.findall(r\"[\\w]+|[.,!?;]\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWpetFO8GHnB"
      },
      "outputs": [],
      "source": [
        "split_words(\"The quick brown fox jumped over the dog.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voc6MqJzGHnB"
      },
      "outputs": [],
      "source": [
        "vocabulary = {\n",
        "    \"[UNK]\": 0,\n",
        "    \"the\": 1,\n",
        "    \"quick\": 2,\n",
        "    \"brown\": 3,\n",
        "    \"fox\": 4,\n",
        "    \"jumped\": 5,\n",
        "    \"over\": 6,\n",
        "    \"dog\": 7,\n",
        "    \".\": 8,\n",
        "}\n",
        "words = split_words(\"The quick brown fox jumped over the lazy dog.\")\n",
        "indices = [vocabulary.get(word, 0) for word in words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyIIN14CGHnB"
      },
      "source": [
        "#### Character and word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tivuAPvnGHnB"
      },
      "outputs": [],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.unk_id = vocabulary[\"[UNK]\"]\n",
        "\n",
        "    def standardize(self, inputs):\n",
        "        return inputs.lower()\n",
        "\n",
        "    def split(self, inputs):\n",
        "        return re.findall(r\".\", inputs)\n",
        "\n",
        "    def index(self, tokens):\n",
        "        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        inputs = self.standardize(inputs)\n",
        "        tokens = self.split(inputs)\n",
        "        indices = self.index(tokens)\n",
        "        return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbrT3e7xGHnC"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def compute_char_vocabulary(inputs, max_size):\n",
        "    char_counts = collections.Counter()\n",
        "    for x in inputs:\n",
        "        x = x.lower()\n",
        "        tokens = re.findall(r\".\", x)\n",
        "        char_counts.update(tokens)\n",
        "    vocabulary = [\"[UNK]\"]\n",
        "    most_common = char_counts.most_common(max_size - len(vocabulary))\n",
        "    for token, count in most_common:\n",
        "        vocabulary.append(token)\n",
        "    return dict((token, i) for i, token in enumerate(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOGKuzggGHnC"
      },
      "outputs": [],
      "source": [
        "class WordTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.unk_id = vocabulary[\"[UNK]\"]\n",
        "\n",
        "    def standardize(self, inputs):\n",
        "        return inputs.lower()\n",
        "\n",
        "    def split(self, inputs):\n",
        "        return re.findall(r\"[\\w]+|[.,!?;]\", inputs)\n",
        "\n",
        "    def index(self, tokens):\n",
        "        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        inputs = self.standardize(inputs)\n",
        "        tokens = self.split(inputs)\n",
        "        indices = self.index(tokens)\n",
        "        return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thjJx9-_GHnC"
      },
      "outputs": [],
      "source": [
        "def compute_word_vocabulary(inputs, max_size):\n",
        "    word_counts = collections.Counter()\n",
        "    for x in inputs:\n",
        "        x = x.lower()\n",
        "        tokens = re.findall(r\"[\\w]+|[.,!?;]\", x)\n",
        "        word_counts.update(tokens)\n",
        "    vocabulary = [\"[UNK]\"]\n",
        "    most_common = word_counts.most_common(max_size - len(vocabulary))\n",
        "    for token, count in most_common:\n",
        "        vocabulary.append(token)\n",
        "    return dict((token, i) for i, token in enumerate(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rNvnk4gGHnC"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "\n",
        "filename = keras.utils.get_file(\n",
        "    origin=\"https://www.gutenberg.org/files/2701/old/moby10b.txt\",\n",
        ")\n",
        "moby_dick = list(open(filename, \"r\"))\n",
        "\n",
        "vocabulary = compute_char_vocabulary(moby_dick, max_size=100)\n",
        "char_tokenizer = CharTokenizer(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVSHHTAeGHnC"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary length:\", len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_xaiRliGHnC"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary start:\", list(vocabulary.keys())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0BYQGZ2GHnC"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary end:\", list(vocabulary.keys())[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dznd-3zKGHnC"
      },
      "outputs": [],
      "source": [
        "print(\"Line length:\", len(char_tokenizer(\n",
        "   \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n",
        ")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfC9ZC0tGHnC"
      },
      "outputs": [],
      "source": [
        "vocabulary = compute_word_vocabulary(moby_dick, max_size=2_000)\n",
        "word_tokenizer = WordTokenizer(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtDEbdyEGHnC"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary length:\", len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnS7TuLNGHnD"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary start:\", list(vocabulary.keys())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Zsbq7AGHnD"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary end:\", list(vocabulary.keys())[-5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSoJ74zwGHnD"
      },
      "outputs": [],
      "source": [
        "print(\"Line length:\", len(word_tokenizer(\n",
        "   \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n",
        ")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmHOt0ALGHnD"
      },
      "source": [
        "#### Subword tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xOKpfqUGHnD"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    \"the quick brown fox\",\n",
        "    \"the slow brown fox\",\n",
        "    \"the quick brown foxhound\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8VmERj5GHnD"
      },
      "outputs": [],
      "source": [
        "def count_and_split_words(data):\n",
        "    counts = collections.Counter()\n",
        "    for line in data:\n",
        "        line = line.lower()\n",
        "        for word in re.findall(r\"[\\w]+|[.,!?;]\", line):\n",
        "            chars = re.findall(r\".\", word)\n",
        "            split_word = \" \".join(chars)\n",
        "            counts[split_word] += 1\n",
        "    return dict(counts)\n",
        "\n",
        "counts = count_and_split_words(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIrPCeCtGHnD"
      },
      "outputs": [],
      "source": [
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgHQfCg4GHnD"
      },
      "outputs": [],
      "source": [
        "def count_pairs(counts):\n",
        "    pairs = collections.Counter()\n",
        "    for word, freq in counts.items():\n",
        "        symbols = word.split()\n",
        "        for pair in zip(symbols[:-1], symbols[1:]):\n",
        "            pairs[pair] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(counts, first, second):\n",
        "    split = re.compile(f\"(?<!\\S){first} {second}(?!\\S)\")\n",
        "    merged = f\"{first}{second}\"\n",
        "    return {split.sub(merged, word): count for word, count in counts.items()}\n",
        "\n",
        "for i in range(10):\n",
        "    pairs = count_pairs(counts)\n",
        "    first, second = max(pairs, key=pairs.get)\n",
        "    counts = merge_pair(counts, first, second)\n",
        "    print(list(counts.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxikPkhGHnD"
      },
      "outputs": [],
      "source": [
        "def compute_sub_word_vocabulary(dataset, vocab_size):\n",
        "    counts = count_and_split_words(dataset)\n",
        "\n",
        "    char_counts = collections.Counter()\n",
        "    for word in counts:\n",
        "        for char in word.split():\n",
        "            char_counts[char] += counts[word]\n",
        "    most_common = char_counts.most_common()\n",
        "    vocab = [\"[UNK]\"] + [char for char, freq in most_common]\n",
        "    merges = []\n",
        "\n",
        "    while len(vocab) < vocab_size:\n",
        "        pairs = count_pairs(counts)\n",
        "        if not pairs:\n",
        "            break\n",
        "        first, second = max(pairs, key=pairs.get)\n",
        "        counts = merge_pair(counts, first, second)\n",
        "        vocab.append(f\"{first}{second}\")\n",
        "        merges.append(f\"{first} {second}\")\n",
        "\n",
        "    vocab = dict((token, index) for index, token in enumerate(vocab))\n",
        "    merges = dict((token, rank) for rank, token in enumerate(merges))\n",
        "    return vocab, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xY50I9VGHnE"
      },
      "outputs": [],
      "source": [
        "class SubWordTokenizer:\n",
        "    def __init__(self, vocabulary, merges):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.merges = merges\n",
        "        self.unk_id = vocabulary[\"[UNK]\"]\n",
        "\n",
        "    def standardize(self, inputs):\n",
        "        return inputs.lower()\n",
        "\n",
        "    def bpe_merge(self, word):\n",
        "        while True:\n",
        "            pairs = re.findall(r\"(?<!\\S)\\S+ \\S+(?!\\S)\", word, overlapped=True)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best = min(pairs, key=lambda pair: self.merges.get(pair, 1e9))\n",
        "            if best not in self.merges:\n",
        "                break\n",
        "            first, second = best.split()\n",
        "            split = re.compile(f\"(?<!\\S){first} {second}(?!\\S)\")\n",
        "            merged = f\"{first}{second}\"\n",
        "            word = split.sub(merged, word)\n",
        "        return word\n",
        "\n",
        "    def split(self, inputs):\n",
        "        tokens = []\n",
        "        for word in re.findall(r\"[\\w]+|[.,!?;]\", inputs):\n",
        "            word = \" \".join(re.findall(r\".\", word))\n",
        "            word = self.bpe_merge(word)\n",
        "            tokens.extend(word.split())\n",
        "        return tokens\n",
        "\n",
        "    def index(self, tokens):\n",
        "        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        inputs = self.standardize(inputs)\n",
        "        tokens = self.split(inputs)\n",
        "        indices = self.index(tokens)\n",
        "        return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLlmgw2yGHnE"
      },
      "outputs": [],
      "source": [
        "vocabulary, merges = compute_sub_word_vocabulary(moby_dick, 2_000)\n",
        "sub_word_tokenizer = SubWordTokenizer(vocabulary, merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh_TXAqFGHnE"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary length:\", len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgPP3yfWGHnE"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary start:\", list(vocabulary.keys())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4ZqoaOnGHnE"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary end:\", list(vocabulary.keys())[-7:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWusrIeLGHnE"
      },
      "outputs": [],
      "source": [
        "print(\"Line length:\", len(sub_word_tokenizer(\n",
        "   \"Call me Ishmael. Some years ago--never mind how long precisely.\"\n",
        ")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UP9eY8PGHnH"
      },
      "source": [
        "### Sets vs. sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sN2CrTDGHnI"
      },
      "source": [
        "#### Loading the IMDb classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qos04x9tGHnI"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "zip_path = keras.utils.get_file(\n",
        "    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "    fname=\"imdb\",\n",
        "    extract=True,\n",
        ")\n",
        "\n",
        "imdb_extract_dir = pathlib.Path(zip_path) / \"aclImdb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFWcRoJxGHnI"
      },
      "outputs": [],
      "source": [
        "for path in imdb_extract_dir.glob(\"*/*\"):\n",
        "    if path.is_dir():\n",
        "        print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk_TogZdGHnI"
      },
      "outputs": [],
      "source": [
        "print(open(imdb_extract_dir / \"train\" / \"pos\" / \"4077_10.txt\", \"r\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxA82s3xGHnI"
      },
      "outputs": [],
      "source": [
        "train_dir = pathlib.Path(\"imdb_train\")\n",
        "test_dir = pathlib.Path(\"imdb_test\")\n",
        "val_dir = pathlib.Path(\"imdb_val\")\n",
        "\n",
        "shutil.copytree(imdb_extract_dir / \"test\", test_dir)\n",
        "\n",
        "val_percentage = 0.2\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    src_dir = imdb_extract_dir / \"train\" / category\n",
        "    src_files = os.listdir(src_dir)\n",
        "    random.Random(1337).shuffle(src_files)\n",
        "    num_val_samples = int(len(src_files) * val_percentage)\n",
        "\n",
        "    os.makedirs(val_dir / category)\n",
        "    for file in src_files[:num_val_samples]:\n",
        "        shutil.copy(src_dir / file, val_dir / category / file)\n",
        "    os.makedirs(train_dir / category)\n",
        "    for file in src_files[num_val_samples:]:\n",
        "        shutil.copy(src_dir / file, train_dir / category / file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dby3OOc-GHnI"
      },
      "outputs": [],
      "source": [
        "from keras.utils import text_dataset_from_directory\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = text_dataset_from_directory(train_dir, batch_size=batch_size)\n",
        "val_ds = text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
        "test_ds = text_dataset_from_directory(test_dir, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYXizxF8GHnI"
      },
      "source": [
        "### Set models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F0cG0arGHnI"
      },
      "source": [
        "#### Training a bag-of-words model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byA4Djv_GHnI"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "max_tokens = 20_000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "train_ds_no_labels = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(train_ds_no_labels)\n",
        "\n",
        "bag_of_words_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")\n",
        "bag_of_words_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")\n",
        "bag_of_words_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WluEWNkfGHnJ"
      },
      "outputs": [],
      "source": [
        "x, y = next(bag_of_words_train_ds.as_numpy_iterator())\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML-OkumTGHnJ"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chgbn1IbGHnJ"
      },
      "outputs": [],
      "source": [
        "def build_linear_classifier(max_tokens, name):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "    model = keras.Model(inputs, outputs, name=name)\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_linear_classifier(max_tokens, \"bag_of_words_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9igx0tRlGHnJ"
      },
      "outputs": [],
      "source": [
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9zlxQRPGHnJ"
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    restore_best_weights=True,\n",
        "    patience=2,\n",
        ")\n",
        "history = model.fit(\n",
        "    bag_of_words_train_ds,\n",
        "    validation_data=bag_of_words_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzVLpbwZGHnJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, accuracy, \"r--\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAKZozoQGHnJ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(bag_of_words_test_ds)\n",
        "test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8pi-zPIGHnJ"
      },
      "source": [
        "#### Training a bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW4-NhtzGHnK"
      },
      "outputs": [],
      "source": [
        "max_tokens = 30_000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"multi_hot\",\n",
        "    ngrams=2,\n",
        ")\n",
        "text_vectorization.adapt(train_ds_no_labels)\n",
        "\n",
        "bigram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")\n",
        "bigram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")\n",
        "bigram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOsU_8PnGHnK"
      },
      "outputs": [],
      "source": [
        "x, y = next(bigram_train_ds.as_numpy_iterator())\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Gw9lanwGHnK"
      },
      "outputs": [],
      "source": [
        "text_vectorization.get_vocabulary()[100:108]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYaAaFcmGHnK"
      },
      "outputs": [],
      "source": [
        "model = build_linear_classifier(max_tokens, \"bigram_classifier\")\n",
        "model.fit(\n",
        "    bigram_train_ds,\n",
        "    validation_data=bigram_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6cvVyfdGHnK"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(bigram_test_ds)\n",
        "test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUu5wKjyGHnK"
      },
      "source": [
        "### Sequence models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la3LQPtbGHnK"
      },
      "outputs": [],
      "source": [
        "max_length = 600\n",
        "max_tokens = 30_000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(train_ds_no_labels)\n",
        "\n",
        "sequence_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")\n",
        "sequence_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")\n",
        "sequence_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu66HPGNGHnK"
      },
      "outputs": [],
      "source": [
        "x, y = next(sequence_test_ds.as_numpy_iterator())\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STruyanYGHnK"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYzpqDCFGHnL"
      },
      "source": [
        "#### Training a recurrent model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-L41uF9GHnL"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "\n",
        "class OneHotEncoding(keras.Layer):\n",
        "    def __init__(self, depth, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "\n",
        "    def call(self, inputs):\n",
        "        flat_inputs = ops.reshape(ops.cast(inputs, \"int\"), [-1])\n",
        "        one_hot_vectors = ops.eye(self.depth)\n",
        "        outputs = ops.take(one_hot_vectors, flat_inputs, axis=0)\n",
        "        return ops.reshape(outputs, ops.shape(inputs) + (self.depth,))\n",
        "\n",
        "one_hot_encoding = OneHotEncoding(max_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0xW0QEYGHnL"
      },
      "outputs": [],
      "source": [
        "x, y = next(sequence_train_ds.as_numpy_iterator())\n",
        "one_hot_encoding(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIu3Yd4NGHnL"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 64\n",
        "inputs = keras.Input(shape=(max_length,), dtype=\"int32\")\n",
        "x = one_hot_encoding(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(hidden_dim))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs, name=\"lstm_with_one_hot\")\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtg5vlUPGHnL"
      },
      "outputs": [],
      "source": [
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCVu7rcMGHnL"
      },
      "outputs": [],
      "source": [
        "# ⚠️NOTE⚠️: The following fit call will error on a T4 GPU on the TensorFlow\n",
        "# backend due to a bug in TensorFlow. If you the follow cell errors out,\n",
        "# do one of the following:\n",
        "# - Skip the following two cells.\n",
        "# - Switch to the Jax or Torch backend and re-run this notebook.\n",
        "# - Change the GPU type in your runtime (requires Colab Pro as of this writing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pANDtxt9GHnL"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    sequence_train_ds,\n",
        "    validation_data=sequence_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6UfbfTHGHnL"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(sequence_test_ds)\n",
        "test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azj91AxBGHnM"
      },
      "source": [
        "#### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXBjWpPbGHnM"
      },
      "source": [
        "#### Using a word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFiHBqePGHnM"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 64\n",
        "inputs = keras.Input(shape=(max_length,), dtype=\"int32\")\n",
        "x = keras.layers.Embedding(\n",
        "    input_dim=max_tokens,\n",
        "    output_dim=hidden_dim,\n",
        "    mask_zero=True,\n",
        ")(inputs)\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(hidden_dim))(x)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs, name=\"lstm_with_embedding\")\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RnDXaCJGHnM"
      },
      "outputs": [],
      "source": [
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMdzsIFgGHnM"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    sequence_train_ds,\n",
        "    validation_data=sequence_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "test_loss, test_acc = model.evaluate(sequence_test_ds)\n",
        "test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oExoVaLGHnM"
      },
      "source": [
        "#### Pretraining a word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X30U-WI_GHnM"
      },
      "outputs": [],
      "source": [
        "imdb_vocabulary = text_vectorization.get_vocabulary()\n",
        "tokenize_no_padding = keras.layers.TextVectorization(\n",
        "    vocabulary=imdb_vocabulary,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycUxECk7GHnM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "context_size = 4\n",
        "window_size = 9\n",
        "\n",
        "def window_data(token_ids):\n",
        "    num_windows = tf.maximum(tf.size(token_ids) - context_size * 2, 0)\n",
        "    windows = tf.range(window_size)[None, :]\n",
        "    windows = windows + tf.range(num_windows)[:, None]\n",
        "    windowed_tokens = tf.gather(token_ids, windows)\n",
        "    return tf.data.Dataset.from_tensor_slices(windowed_tokens)\n",
        "\n",
        "def split_label(window):\n",
        "    left = window[:context_size]\n",
        "    right = window[context_size + 1 :]\n",
        "    bag = tf.concat((left, right), axis=0)\n",
        "    label = window[4]\n",
        "    return bag, label\n",
        "\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    imdb_extract_dir / \"train\", batch_size=None\n",
        ")\n",
        "dataset = dataset.map(lambda x, y: x, num_parallel_calls=8)\n",
        "dataset = dataset.map(tokenize_no_padding, num_parallel_calls=8)\n",
        "dataset = dataset.interleave(window_data, cycle_length=8, num_parallel_calls=8)\n",
        "dataset = dataset.map(split_label, num_parallel_calls=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj7PJg0YGHnM"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 64\n",
        "inputs = keras.Input(shape=(2 * context_size,))\n",
        "cbow_embedding = layers.Embedding(\n",
        "    max_tokens,\n",
        "    hidden_dim,\n",
        ")\n",
        "x = cbow_embedding(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(max_tokens, activation=\"sigmoid\")(x)\n",
        "cbow_model = keras.Model(inputs, outputs)\n",
        "cbow_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3PHAeZGGHnN"
      },
      "outputs": [],
      "source": [
        "cbow_model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDK70h_jGHnN"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.batch(1024).cache()\n",
        "cbow_model.fit(dataset, epochs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIZNLN-zGHnN"
      },
      "source": [
        "#### Using the pretrained embedding for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTsjEMyOGHnN"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(max_length,))\n",
        "lstm_embedding = layers.Embedding(\n",
        "    input_dim=max_tokens,\n",
        "    output_dim=hidden_dim,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = lstm_embedding(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(hidden_dim))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs, name=\"lstm_with_cbow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGGp-OidGHnN"
      },
      "outputs": [],
      "source": [
        "lstm_embedding.embeddings.assign(cbow_embedding.embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcTYznK7GHnN"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(\n",
        "    sequence_train_ds,\n",
        "    validation_data=sequence_val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auf6FyesGHnN"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(sequence_test_ds)\n",
        "test_acc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}